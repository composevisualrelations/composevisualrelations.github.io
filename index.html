<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LP32F6JVKW"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());
        gtag('config', 'G-LP32F6JVKW');

       function setImage(select){
           var image = document.getElementsByName("image-swap")[0];
           image.src = select.options[select.selectedIndex].value;
        }
    </script>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css"
          integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
    <!-- Custom styles for this template -->
    <link href="files/jumbotron.css" rel="stylesheet">

    <style type="text/css">
        a:hover {
            color: #000;
        }

        a::before {
            content: "";
            position: absolute;
            display: block;
            width: 100%;
            height: 1px;
            bottom: 0;
            left: 0;
            background-color: #000;
            transform: scaleX(0);
            transition: transform 0.3s ease;
        }
        select {
          /* for Firefox */
          -moz-appearance: none;
          /* for Chrome */
          -webkit-appearance: none;
        }
        .dropselect {
            background-repeat: repeat-x;
            background-size: 1px 1px;
            background-position: 0 85%;
            border: none;
            outline: none;
            scroll-behavior: smooth;
            border-bottom:solid 1px #000;
            width:auto
        }
        .dropselect option {
          color: initial; /* fix Windows bug */
        }
    </style>
    <title>Learning to Compose Visual Relations</title>

</head>

<title>Learning to Compose Visual Relations</title>

<body>
<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
    <a class="navbar-brand" href="#">Learning to Compose Visual Relations</a>

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarToggle">
        <ul class="navbar-nav ml-auto">
            <li class="nav-item">
                <a class="nav-link" href="#">Home</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#Model">Model</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#Abstract">Abstract</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#Paper">Paper</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#Results">Results</a>
            </li>
        </ul>
    </div>
</nav>

<div class="container" style="padding-top: 80px; font-size: 20px">
    <div align="center">
        <h2 class="text-center" align="center">
            <p>Learning to Compose Visual Relations</p>
        </h2>
        <h3 class="text-center">
            <p style="color: #fe5a1d">NeurIPS 2021 (Spotlight)</p>
        </h3>
        <a href="https://nanliu.io/">Nan Liu</a><sup>1*</sup> &nbsp;
        <a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a><sup>2*</sup> &nbsp;
        <a href="https://yilundu.github.io">Yilun Du</a><sup>2*</sup> &nbsp;
        <a href="https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/">Joshua B. Tenenbaum</a><sup>2</sup> &nbsp;
        <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a><sup>2</sup>

        <br><br>
        <a href="https://eecs.engin.umich.edu/"><b>University of Michigan</b></a>&nbsp;&nbsp;&nbsp;
        <a href="https://www.csail.mit.edu/"><b>MIT CSAIL</b></a><br>
        <small>(* indicate equal contribution)</small>

    </div>
</div>
<br>

<div class="container">
    <h3 id="Model" style="padding-top: 80px; margin-top: -80px;">Model Overview</h3>
    <div class="row align-items-center">
        <div class="col justify-content-center text-center">
            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/model.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <hr class="my-4">
</div>
<br>

<!-- Abstract -->
<div class="container">
    <h3 id="Abstract" style="padding-top: 80px; margin-top: -80px;">Abstract</h3>
    The visual world around us can be described as a structured set of objects and their associated relations.
    An image of a room may be conjured given only the description of the underlying objects and their associated relations.
    While there has been significant work on designing deep neural networks which may compose individual objects together,
    less work has been done on composing the individual relations between objects.
    A principal difficulty is that while the placement of objects is mutually independent,
    their relations are entangled and dependent on each other.
    To circumvent this issue, existing works primarily compose relations by utilizing a holistic encoder, in the form of text or graphs.
    In this work, we instead propose to represent each relation as an unnormalized density (an energy-based model),
    enabling us to compose separate relations in a factorized manner.
    We show that such a factorized decomposition allows the model to both generate and edit scenes that have multiple sets of relations more faithfully.
    We further show that decomposition enables our model to effectively understand the underlying relational scene structure.
</div>
<br><br>

<!-- Paper -->
<div class="container">
    <h3 id="Paper" style="padding-top: 80px; margin-top: -80px;">Paper</h3>

    <div class="row">
        <div class="col-md-12">
            <a href="https://nanliu.io/">Nan Liu</a><sup>1*</sup> &nbsp;
            <a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a><sup>2*</sup> &nbsp;
            <a href="https://yilundu.github.io">Yilun Du</a><sup>2*</sup> &nbsp;
            <a href="https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/">Joshua B. Tenenbaum</a><sup>2</sup>
            &nbsp;
            <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a><sup>2</sup><br>

            <b>Learning to Compose Visual Relations</b><br>

            <b>NeurIPS 2021 (Spotlight)</b>
            <a href="#" target="_blank">[Code]</a>
            <a href="#">[Paper]</a>
            <a href="#" target="_blank">[BibTex][Coming soon]</a><br>
            <!--            Abridged in <b>NeurIPS 2021</b> workshop on Visual Learning and Reasoning for Robotics <a href="https://rssvlrr.github.io/">[Link]</a><br>-->
            (* indicate equal contribution)
        </div>
    </div>
</div>
<br><br>

<div class="container">
    <h3 id="Images" style="padding-top: 80px; margin-top: -80px;">Illustrations</h3><br>
    <div class="section" style="text-align: center">
        <span><strong>Text Prompt: </strong></span>
        <span style="color: #AAAAAA"><strong>A small gray rubber cube</strong></span>
        <label for="rel">
            <select id='rel' class="dropselect" style="text-align: center" name="clevr_generation_1" onchange="setImage(this);">
                <option value="img/clevr_generation_1_relation/a small gray rubber cube to the left of a small purple metal cube.png">to the left of</option>
                <option value="img/clevr_generation_1_relation/a small gray rubber cube to the right of a small purple metal cube.png" selected="">to the right of</option>
                <option value="img/clevr_generation_1_relation/a small gray rubber cube above a small purple metal cube.png">above</option>
                <option value="img/clevr_generation_1_relation/a small gray rubber cube below a small purple metal cube.png">below</option>
                <option value="img/clevr_generation_1_relation/a small gray rubber cube in front of a small purple metal cube.png">in front of</option>
                <option value="img/clevr_generation_1_relation/a small gray rubber cube behind a small purple metal cube.png">behind</option>
            </select>
        </label>
        <span style="color: #B10DC9"><strong>a small purple metal cube</strong></span>
    </div>
    <div class="section">
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/clevr_generation_1_relation/a small gray rubber cube to the right of a small purple metal cube.png"
                     class="img-fluid" alt="" style="width:50%; height:60%" name="image-swap">
            </div>
        </div>
    </div>
    <hr class="my-4">
</div>

<div class="container">
    <h3 id="Results" style="padding-top: 80px; margin-top: -80px;">Results</h3><br>
    <div class="section">
        <h4 class="text-center">Image Generation</h4>
        <hr class="my-4">
        <p>
            Image generation results on the <b>CLEVR</b> dataset.
            Image are generated based on <b>1 - 4</b> relational descriptions.
            Note that the models are trained on a single relational description and the composed scene relations
            <b>(2, 3, and 4 relational descriptions) are outside the training distribution</b>.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/generation_clevr.png" class="img-fluid" alt="" style="width:90%; height:100%">
            </div>
        </div>
        <hr class="my-4">
        <p>
            We further test on <b>iGibson</b> dataset. Below are generated iGibson images.
            Note that <b>2 relational description is outside the training distribution</b>.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/generation_igibson.png" class="img-fluid" alt="" style="width:90%; height:100%">
            </div>
        </div>
        <hr class="my-4">
        <p>
            Lastly, we test on real-world blocks dataset from <a href="https://arxiv.org/pdf/1603.01312.pdf">https://arxiv.org/pdf/1603.01312.pdf</a>.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/generation_block.png" class="img-fluid" alt="" style="width:90%; height:100%">
            </div>
        </div>
    </div>
    <br>
    <br>

    <div class="section">
        <h4 class="text-center">Image Editing</h4>
        <hr class="my-4">
        <p> Given an input image, we can edit this image based on inverse relations (i.e., left to right).
        </p>
         <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/editing_clevr.png" class="img-fluid" alt="" style="width:90%; height:100%">
            </div>
        </div>
    </div>
    <br>
    <br>

    <div class="section">
        <h4 class="text-center">Image-text Retrieval</h4>
        <hr class="my-4">
        <p>We hypothesize the good generation performance of our proposed approach is due to our system's understanding of
            relations and ability to distinguish between different relational scene descriptions.
            In this section, we evaluate the relational understanding ability of our proposed method and baselines by comparing their image-to-text retrieval performance.
        </p>
         <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/retrieval.png" class="img-fluid" alt="" style="width:90%; height:100%">
            </div>
        </div>
    </div>
    <br>
    <br>

    <div class="section">
        <h4 class="text-center">Generalization</h4>
        <hr class="my-4">
        <p>To quantitatively evaluate the generalization ability across datasets of the proposed method,
            we test the <b>image-to-text retrieval</b> accuracy on the Blender dataset that contains 300 pairs of images and relational scene descriptions.
        </p>
         <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/zero_shot_generalization.png" class="img-fluid" alt="" style="width:50%; height:100%">
            </div>
        </div>
    </div>
    <br>
    <br>

    <div class="section">
        <h4 class="text-center">Additional results</h4>
        <hr class="my-4">
        <p>We provide qualitative comparisons of different methods.
            Firstly, we show a comprehensive comparison on <b>Clevr</b> dataset.
        </p>
         <div class="row align-items-center">
             <div class="col justify-content-center text-center">
                <img src="img/image_generation_clevr_2_appendix.png" class="img-fluid" alt="" style="width:90%; height:100%">
            </div>
        </div>

        <hr class="my-4">
        <p>Then below we show comparisons on <b>iGibson</b> dataset.
        </p>
        <div class="row align-items-center">
             <div class="col justify-content-center text-center">
                <img src="img/image_generation_igibson_2_appendix.png" class="img-fluid" alt="" style="width:90%; height:100%">
            </div>
        </div>

        <hr class="my-4">
        <p>Next, we investigate whether our approach <b>Ours (Learned Emb)</b> underperform in real scenes
            since there will be more complex relations, attributes, and objects for real scenes.
            Below, we show image generation on real-world Blocks dataset. <b>Ours (Learned Emb)</b> still outperforms
            the other two baselines.
        </p>
        <div class="row align-items-center">
             <div class="col justify-content-center text-center">
                <img src="img/image_generation_block_appendix.png" class="img-fluid" alt="" style="width:90%; height:100%">
            </div>
        </div>
    </div>
</div>
<br><br>


<div class="container">
    <hr>
    <center>
        <footer>
            <p>&copy; Massachusetts Institute of Technology 2021</p>
        </footer>
    </center>
</div>

<!-- Bootstrap core JavaScript -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
        integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
        crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"
        integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1"
        crossorigin="anonymous"></script>
</body>

</html>